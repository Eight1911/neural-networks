{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poom C's Char-LSTM Langauge Detector\n",
    "---\n",
    "This is an char-LSTM for language classifier. The model is standard LSTMs with Dropout and added input noise. The data is taken from *European Parliament Proceedings Parallel Corpus 1996-2011* and the test set is obtained from https://fellowship.ai/challenge/ in late 2017. (Sounds like a macbook pro lol.) The code explains itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reader(directory):\n",
    "    \"\"\"\n",
    "    Takes \n",
    "        - directory : string\n",
    "            the directory path of all the text files\n",
    "    Returns \n",
    "        - dict[string -> list[string]]\n",
    "            a dictionary whose keys are the languages (folder-name)\n",
    "            and values are list of strings in the files in the folder\n",
    "    \"\"\"\n",
    "    def parse(base_name, files):\n",
    "        print(\"parsing files in\", base_name)\n",
    "        pattern = re.compile('<[^>]*>')\n",
    "        strings = []\n",
    "        for name in files:\n",
    "            with open(base_name + \"/\"+ name, errors='replace') as f:\n",
    "                strings.append(re.sub(pattern, '', f.read().strip()))\n",
    "        return strings\n",
    "\n",
    "    def main(directory):\n",
    "        scanner = os.walk(directory)\n",
    "        _, folders, _ = next(scanner)\n",
    "        return {name: parse(name, files)\n",
    "                for name, _, files in scanner}\n",
    "    \n",
    "    return main(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we parse the files. This will take sometime. Go get coffee or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing files in ./text/bg\n",
      "parsing files in ./text/cs\n",
      "parsing files in ./text/da\n",
      "parsing files in ./text/de\n",
      "parsing files in ./text/el\n",
      "parsing files in ./text/en\n",
      "parsing files in ./text/es\n",
      "parsing files in ./text/et\n",
      "parsing files in ./text/fi\n",
      "parsing files in ./text/fr\n",
      "parsing files in ./text/hu\n",
      "parsing files in ./text/it\n",
      "parsing files in ./text/lt\n",
      "parsing files in ./text/lv\n",
      "parsing files in ./text/nl\n",
      "parsing files in ./text/pl\n",
      "parsing files in ./text/pt\n",
      "parsing files in ./text/ro\n",
      "parsing files in ./text/sk\n",
      "parsing files in ./text/sl\n",
      "parsing files in ./text/sv\n"
     ]
    }
   ],
   "source": [
    "strings = reader(\"./text\")\n",
    "joined = { lang: \" | \".join(strings[lang])\n",
    "            for lang in strings }\n",
    "charset = { lang: set(joined[lang]) for lang in joined }\n",
    "languages = sorted(charset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of characters: 4499444944\n"
     ]
    }
   ],
   "source": [
    "num_chars = sum(len(corp) for corp in joined.values())\n",
    "print(\"total number of characters:\", num_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(joined, window_size, char_to_ind, error_rate=0.001):\n",
    "    error_ind = char_to_ind[\"error\"]\n",
    "    languages = sorted(joined)\n",
    "    labels = [np.repeat(i, window_size) for i in range(len(languages))]\n",
    "    \n",
    "    def next_point():\n",
    "        index = random.randrange(len(languages))\n",
    "        string = joined[languages[index]]\n",
    "        start = random.randrange(0, len(string) - window_size)\n",
    "        substring = string[start:start+window_size]\n",
    "        \n",
    "        # add random all upper case and all lower case data \n",
    "        #rand = random.random() \n",
    "        #if rand > 0.9:\n",
    "        #    substring = substring.lower()[:window_size]\n",
    "        # if rand < 0.05:\n",
    "        #     substring = substring.upper()[:window_size]\n",
    "        inputs = np.array([char_to_ind[c] for c in substring])\n",
    "        if error_rate > 0:\n",
    "            error = rd.binomial(1, error_rate, len(substring))\n",
    "            mask = error.astype(bool)\n",
    "            inputs[mask] = error_ind\n",
    "\n",
    "        return labels[index], inputs\n",
    "\n",
    "    def iterate(batch_size):\n",
    "        while True:\n",
    "            data = [next_point() for _ in range(batch_size)]\n",
    "            labels, inputs = zip(*data)\n",
    "            yield np.vstack(labels), np.array(inputs)\n",
    "\n",
    "    return iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497 unique characters\n"
     ]
    }
   ],
   "source": [
    "unique_char = set([\"error\"])\n",
    "for lang in charset:\n",
    "    unique_char.update(charset[lang])\n",
    "#unique_char.update([i for c in unique_char for i in c.upper() ])\n",
    "#unique_char.update([i for c in unique_char for i in c.lower() ])\n",
    "unique_char = sorted(unique_char)\n",
    "num_unique_chars = len(unique_char)\n",
    "char_to_ind = { c : i for i, c in enumerate(unique_char) }\n",
    "print(num_unique_chars, \"unique characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a standard Char-LSTM classifier for this problem. That is, we embed characters into $\\mathbb{R}^n$, feed the sequence of embedded points into the RNN and have each output be measured against the one-hot language vector using KL divergence. \n",
    "\n",
    "We also use dropout in the input layer to improve generalizability and add a 1% random error in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_size = 40\n",
    "num_layers = 2\n",
    "batch_size = 30\n",
    "window_size = 130\n",
    "output_size = 21\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_ids = tf.placeholder(tf.int64, [batch_size, window_size])\n",
    "labels   = tf.placeholder(tf.int64, [batch_size, window_size])\n",
    "pkeep    = tf.placeholder(tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.linspace(0, 1, window_size) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xavier initialization\n",
    "embedding = rd.randn(num_unique_chars, rnn_size)\n",
    "embedding = tf.Variable(embedding / np.sqrt(rnn_size + rnn_size))\n",
    "inp = tf.nn.embedding_lookup(embedding, char_ids)\n",
    "\n",
    "# xavier initialization\n",
    "noise_prob = tf.Variable(np.eye(output_size))\n",
    "decoder = rd.randn(rnn_size, output_size)\n",
    "decoder = tf.Variable(decoder / np.sqrt(rnn_size + output_size))\n",
    "bias    = tf.zeros(output_size, tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell = lambda size: (\n",
    "    tf.nn.rnn_cell.DropoutWrapper\n",
    "   (tf.nn.rnn_cell.LSTMCell(size), pkeep))\n",
    "# lstm_cell = tf.nn.rnn_cell.LSTMCell\n",
    "cells = [lstm_cell(rnn_size) for _ in range(num_layers)]\n",
    "lstm = tf.nn.rnn_cell.MultiRNNCell(cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = lstm.zero_state(batch_size, tf.float64)\n",
    "outputs = []\n",
    "for i in range(window_size):\n",
    "    output, state = lstm(inp[:,i,:], state)\n",
    "    outputs.append(output)\n",
    "\n",
    "outputs = tf.stack(outputs, axis=1)\n",
    "outputs = tf.tensordot(outputs, decoder, [[2], [0]]) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_index = 100\n",
    "prediction = tf.argmax(outputs[:, len_index], axis=1),\n",
    "accuracy = tf.reduce_sum(tf.cast(tf.equal(prediction, labels[:, len_index]), tf.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the next two cells will take some time. please go get another cup of coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "longname = tf.nn.sparse_softmax_cross_entropy_with_logits # lol\n",
    "loss_matrix = longname(logits=outputs, labels=labels)\n",
    "total_loss = tf.reduce_mean(weights * loss_matrix)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "minimizer = optimizer.minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we are training, this cell will also take very long, go get two more cups of coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 with decaying accuracy 0.3465\n",
      "iteration 200 with decaying accuracy 0.219156050329\n",
      "iteration 400 with decaying accuracy 0.288316617281\n",
      "iteration 600 with decaying accuracy 0.392254653404\n",
      "iteration 800 with decaying accuracy 0.496579498338\n",
      "iteration 1000 with decaying accuracy 0.603866247323\n",
      "iteration 1200 with decaying accuracy 0.66292606433\n",
      "iteration 1400 with decaying accuracy 0.730951091785\n",
      "iteration 1600 with decaying accuracy 0.776574085996\n",
      "iteration 1800 with decaying accuracy 0.817587951\n",
      "iteration 2000 with decaying accuracy 0.828428721345\n",
      "iteration 2200 with decaying accuracy 0.863256274586\n",
      "iteration 2400 with decaying accuracy 0.875754084744\n",
      "iteration 2600 with decaying accuracy 0.883066352971\n",
      "iteration 2800 with decaying accuracy 0.89473352237\n",
      "iteration 3000 with decaying accuracy 0.91309197695\n",
      "iteration 3200 with decaying accuracy 0.917762144954\n",
      "iteration 3400 with decaying accuracy 0.887860895696\n",
      "iteration 3600 with decaying accuracy 0.920016514713\n",
      "iteration 3800 with decaying accuracy 0.926101570591\n",
      "iteration 4000 with decaying accuracy 0.931641115143\n",
      "iteration 4200 with decaying accuracy 0.936118446207\n",
      "iteration 4400 with decaying accuracy 0.936355435995\n",
      "iteration 4600 with decaying accuracy 0.936208478223\n",
      "iteration 4800 with decaying accuracy 0.940263993758\n",
      "iteration 5000 with decaying accuracy 0.946282634635\n",
      "iteration 5200 with decaying accuracy 0.945655203735\n",
      "iteration 5400 with decaying accuracy 0.946697425034\n",
      "iteration 5600 with decaying accuracy 0.942883153043\n",
      "iteration 5800 with decaying accuracy 0.943593482289\n",
      "iteration 6000 with decaying accuracy 0.944594087044\n",
      "iteration 6200 with decaying accuracy 0.950250647325\n",
      "iteration 6400 with decaying accuracy 0.946446577475\n",
      "iteration 6600 with decaying accuracy 0.94716587056\n",
      "iteration 6800 with decaying accuracy 0.951335807669\n",
      "iteration 7000 with decaying accuracy 0.949722257793\n",
      "iteration 7200 with decaying accuracy 0.951017344629\n",
      "iteration 7400 with decaying accuracy 0.953826065876\n",
      "iteration 7600 with decaying accuracy 0.953833102694\n",
      "iteration 7800 with decaying accuracy 0.952992390222\n",
      "iteration 8000 with decaying accuracy 0.957110751499\n",
      "iteration 8200 with decaying accuracy 0.960423101585\n",
      "iteration 8400 with decaying accuracy 0.956871008709\n",
      "iteration 8600 with decaying accuracy 0.96012202088\n",
      "iteration 8800 with decaying accuracy 0.958414069064\n",
      "iteration 9000 with decaying accuracy 0.958237367913\n",
      "iteration 9200 with decaying accuracy 0.95989731408\n",
      "iteration 9400 with decaying accuracy 0.959962120704\n",
      "iteration 9600 with decaying accuracy 0.960194526173\n",
      "iteration 9800 with decaying accuracy 0.960827740795\n",
      "iteration 10000 with decaying accuracy 0.961417025569\n",
      "iteration 10200 with decaying accuracy 0.961466407944\n",
      "iteration 10400 with decaying accuracy 0.962696195788\n",
      "iteration 10600 with decaying accuracy 0.961884912012\n",
      "iteration 10800 with decaying accuracy 0.959461551647\n",
      "iteration 11000 with decaying accuracy 0.959983448859\n",
      "iteration 11200 with decaying accuracy 0.966538962985\n",
      "iteration 11400 with decaying accuracy 0.964776980965\n",
      "iteration 11600 with decaying accuracy 0.966968155154\n",
      "iteration 11800 with decaying accuracy 0.967804898305\n",
      "iteration 12000 with decaying accuracy 0.967285917678\n",
      "iteration 12200 with decaying accuracy 0.967370711574\n",
      "iteration 12400 with decaying accuracy 0.971105823081\n",
      "iteration 12600 with decaying accuracy 0.968913461883\n",
      "iteration 12800 with decaying accuracy 0.973095789243\n",
      "iteration 13000 with decaying accuracy 0.972202989391\n",
      "iteration 13200 with decaying accuracy 0.978338273474\n",
      "iteration 13400 with decaying accuracy 0.976458510104\n",
      "iteration 13600 with decaying accuracy 0.975102163137\n",
      "iteration 13800 with decaying accuracy 0.970267606103\n",
      "iteration 14000 with decaying accuracy 0.975321248892\n",
      "iteration 14200 with decaying accuracy 0.977419247006\n",
      "iteration 14400 with decaying accuracy 0.977821497283\n",
      "iteration 14600 with decaying accuracy 0.973205822191\n",
      "iteration 14800 with decaying accuracy 0.974867854947\n",
      "iteration 15000 with decaying accuracy 0.972150808233\n",
      "iteration 15200 with decaying accuracy 0.975973213214\n",
      "iteration 15400 with decaying accuracy 0.975595770868\n",
      "iteration 15600 with decaying accuracy 0.9796450317\n",
      "iteration 15800 with decaying accuracy 0.980478293306\n",
      "iteration 16000 with decaying accuracy 0.975050630782\n",
      "iteration 16200 with decaying accuracy 0.982414379331\n",
      "iteration 16400 with decaying accuracy 0.979577664219\n",
      "iteration 16600 with decaying accuracy 0.978781708766\n",
      "iteration 16800 with decaying accuracy 0.978684053944\n",
      "iteration 17000 with decaying accuracy 0.978065181332\n",
      "iteration 17200 with decaying accuracy 0.979764978154\n",
      "iteration 17400 with decaying accuracy 0.983736934121\n",
      "iteration 17600 with decaying accuracy 0.983093827147\n",
      "iteration 17800 with decaying accuracy 0.981994622167\n",
      "iteration 18000 with decaying accuracy 0.98047355227\n",
      "iteration 18200 with decaying accuracy 0.980870402231\n",
      "iteration 18400 with decaying accuracy 0.981697002665\n",
      "iteration 18600 with decaying accuracy 0.981818507989\n",
      "iteration 18800 with decaying accuracy 0.98222427561\n",
      "iteration 19000 with decaying accuracy 0.98209545712\n",
      "iteration 19200 with decaying accuracy 0.982243385231\n",
      "iteration 19400 with decaying accuracy 0.980660298938\n",
      "iteration 19600 with decaying accuracy 0.982030832019\n",
      "iteration 19800 with decaying accuracy 0.986101855148\n",
      "iteration 20000 with decaying accuracy 0.98206985255\n",
      "iteration 20200 with decaying accuracy 0.984925769949\n",
      "iteration 20400 with decaying accuracy 0.986809027325\n",
      "iteration 20600 with decaying accuracy 0.988006209679\n",
      "iteration 20800 with decaying accuracy 0.984826528484\n",
      "iteration 21000 with decaying accuracy 0.98626389298\n",
      "iteration 21200 with decaying accuracy 0.986066630624\n",
      "iteration 21400 with decaying accuracy 0.984493837548\n",
      "iteration 21600 with decaying accuracy 0.986986411718\n",
      "iteration 21800 with decaying accuracy 0.984533733503\n",
      "iteration 22000 with decaying accuracy 0.987229508788\n",
      "iteration 22200 with decaying accuracy 0.987522249424\n",
      "iteration 22400 with decaying accuracy 0.988169367395\n",
      "iteration 22600 with decaying accuracy 0.987111324677\n",
      "iteration 22800 with decaying accuracy 0.98775187517\n",
      "iteration 23000 with decaying accuracy 0.988203970485\n",
      "iteration 23200 with decaying accuracy 0.989314088311\n",
      "iteration 23400 with decaying accuracy 0.988432590222\n",
      "iteration 23600 with decaying accuracy 0.988330105783\n",
      "iteration 23800 with decaying accuracy 0.987992703995\n",
      "iteration 24000 with decaying accuracy 0.990479367623\n",
      "iteration 24200 with decaying accuracy 0.989557108637\n",
      "iteration 24400 with decaying accuracy 0.990089583196\n",
      "iteration 24600 with decaying accuracy 0.989314163354\n",
      "iteration 24800 with decaying accuracy 0.988265643669\n",
      "iteration 25000 with decaying accuracy 0.987742063277\n",
      "iteration 25200 with decaying accuracy 0.986107975155\n",
      "iteration 25400 with decaying accuracy 0.98950542401\n",
      "iteration 25600 with decaying accuracy 0.987936230013\n",
      "iteration 25800 with decaying accuracy 0.988232689798\n",
      "iteration 26000 with decaying accuracy 0.98843964042\n",
      "iteration 26200 with decaying accuracy 0.989470461775\n",
      "iteration 26400 with decaying accuracy 0.990928297619\n",
      "iteration 26600 with decaying accuracy 0.988129137985\n",
      "iteration 26800 with decaying accuracy 0.988436520677\n",
      "iteration 27000 with decaying accuracy 0.991866131502\n",
      "iteration 27200 with decaying accuracy 0.991338620922\n",
      "iteration 27400 with decaying accuracy 0.992621484715\n",
      "iteration 27600 with decaying accuracy 0.989091501236\n",
      "iteration 27800 with decaying accuracy 0.990751985535\n",
      "iteration 28000 with decaying accuracy 0.990828914063\n",
      "iteration 28200 with decaying accuracy 0.991111323363\n",
      "iteration 28400 with decaying accuracy 0.992569918448\n",
      "iteration 28600 with decaying accuracy 0.991527758339\n",
      "iteration 28800 with decaying accuracy 0.992766113158\n",
      "iteration 29000 with decaying accuracy 0.993655789214\n",
      "iteration 29200 with decaying accuracy 0.991754480675\n",
      "iteration 29400 with decaying accuracy 0.990687842938\n",
      "iteration 29600 with decaying accuracy 0.990528890089\n",
      "iteration 29800 with decaying accuracy 0.992094218324\n",
      "iteration 30000 with decaying accuracy 0.993546502692\n",
      "iteration 30200 with decaying accuracy 0.993371014712\n",
      "iteration 30400 with decaying accuracy 0.991651290403\n",
      "iteration 30600 with decaying accuracy 0.992439492563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 30800 with decaying accuracy 0.990992493372\n",
      "iteration 31000 with decaying accuracy 0.992024040506\n",
      "iteration 31200 with decaying accuracy 0.993733747732\n",
      "iteration 31400 with decaying accuracy 0.991669427209\n",
      "iteration 31600 with decaying accuracy 0.989078372029\n",
      "iteration 31800 with decaying accuracy 0.993417817835\n",
      "iteration 32000 with decaying accuracy 0.99155312685\n",
      "iteration 32200 with decaying accuracy 0.991617058574\n",
      "iteration 32400 with decaying accuracy 0.991715510666\n",
      "iteration 32600 with decaying accuracy 0.994363453533\n",
      "iteration 32800 with decaying accuracy 0.991897923867\n",
      "iteration 33000 with decaying accuracy 0.994086451884\n",
      "iteration 33200 with decaying accuracy 0.994678248978\n",
      "iteration 33400 with decaying accuracy 0.993558586297\n",
      "iteration 33600 with decaying accuracy 0.992067865333\n",
      "iteration 33800 with decaying accuracy 0.994061549462\n",
      "iteration 34000 with decaying accuracy 0.993354147979\n",
      "iteration 34200 with decaying accuracy 0.993570602097\n",
      "iteration 34400 with decaying accuracy 0.994180312755\n",
      "iteration 34600 with decaying accuracy 0.994049603209\n",
      "iteration 34800 with decaying accuracy 0.99451412545\n",
      "iteration 35000 with decaying accuracy 0.991119645746\n",
      "iteration 35200 with decaying accuracy 0.993100869986\n",
      "iteration 35400 with decaying accuracy 0.993389221634\n",
      "iteration 35600 with decaying accuracy 0.992798469645\n",
      "iteration 35800 with decaying accuracy 0.993617995286\n",
      "iteration 36000 with decaying accuracy 0.994183694781\n",
      "iteration 36200 with decaying accuracy 0.991410867464\n",
      "iteration 36400 with decaying accuracy 0.992659504426\n",
      "iteration 36600 with decaying accuracy 0.994206401886\n",
      "iteration 36800 with decaying accuracy 0.993198153815\n",
      "iteration 37000 with decaying accuracy 0.993438303046\n",
      "iteration 37200 with decaying accuracy 0.993782952338\n",
      "iteration 37400 with decaying accuracy 0.99399417036\n",
      "iteration 37600 with decaying accuracy 0.99286453366\n",
      "iteration 37800 with decaying accuracy 0.992815901998\n",
      "iteration 38000 with decaying accuracy 0.994595593381\n",
      "iteration 38200 with decaying accuracy 0.996454424996\n",
      "iteration 38400 with decaying accuracy 0.995850727452\n",
      "iteration 38600 with decaying accuracy 0.996233188049\n",
      "iteration 38800 with decaying accuracy 0.996731936731\n",
      "iteration 39000 with decaying accuracy 0.995475933124\n",
      "iteration 39200 with decaying accuracy 0.995459843022\n",
      "iteration 39400 with decaying accuracy 0.99603604778\n",
      "iteration 39600 with decaying accuracy 0.992640279797\n",
      "iteration 39800 with decaying accuracy 0.994960421845\n",
      "iteration 40000 with decaying accuracy 0.994946701987\n",
      "iteration 40200 with decaying accuracy 0.994338102624\n",
      "iteration 40400 with decaying accuracy 0.996120495505\n",
      "iteration 40600 with decaying accuracy 0.995299828469\n",
      "iteration 40800 with decaying accuracy 0.995756790954\n",
      "iteration 41000 with decaying accuracy 0.993790673811\n",
      "iteration 41200 with decaying accuracy 0.993660036765\n",
      "iteration 41400 with decaying accuracy 0.995550552785\n",
      "iteration 41600 with decaying accuracy 0.99594549494\n",
      "iteration 41800 with decaying accuracy 0.995399472485\n",
      "iteration 42000 with decaying accuracy 0.995690789503\n",
      "iteration 42200 with decaying accuracy 0.993353125622\n",
      "iteration 42400 with decaying accuracy 0.993876897608\n",
      "iteration 42600 with decaying accuracy 0.995037260002\n",
      "iteration 42800 with decaying accuracy 0.99568266206\n",
      "iteration 43000 with decaying accuracy 0.995956031061\n",
      "iteration 43200 with decaying accuracy 0.995653312171\n",
      "iteration 43400 with decaying accuracy 0.996465018495\n",
      "iteration 43600 with decaying accuracy 0.995436360879\n",
      "iteration 43800 with decaying accuracy 0.994953988712\n",
      "iteration 44000 with decaying accuracy 0.993826667391\n",
      "iteration 44200 with decaying accuracy 0.994671697845\n",
      "iteration 44400 with decaying accuracy 0.99310215983\n",
      "iteration 44600 with decaying accuracy 0.992738469199\n",
      "iteration 44800 with decaying accuracy 0.994949493986\n",
      "iteration 45000 with decaying accuracy 0.992587424883\n",
      "iteration 45200 with decaying accuracy 0.996093592589\n",
      "iteration 45400 with decaying accuracy 0.995452749264\n",
      "iteration 45600 with decaying accuracy 0.993566030617\n",
      "iteration 45800 with decaying accuracy 0.994145355796\n",
      "iteration 46000 with decaying accuracy 0.996908137751\n",
      "iteration 46200 with decaying accuracy 0.996201402447\n",
      "iteration 46400 with decaying accuracy 0.997216612025\n",
      "iteration 46600 with decaying accuracy 0.994643726686\n",
      "iteration 46800 with decaying accuracy 0.995526086602\n",
      "iteration 47000 with decaying accuracy 0.996853468816\n",
      "iteration 47200 with decaying accuracy 0.99620662221\n",
      "iteration 47400 with decaying accuracy 0.995333324037\n",
      "iteration 47600 with decaying accuracy 0.995927405718\n",
      "iteration 47800 with decaying accuracy 0.994841172548\n",
      "iteration 48000 with decaying accuracy 0.995883020731\n",
      "iteration 48200 with decaying accuracy 0.993663805182\n",
      "iteration 48400 with decaying accuracy 0.993630478498\n",
      "iteration 48600 with decaying accuracy 0.996759244192\n",
      "iteration 48800 with decaying accuracy 0.996570507144\n",
      "iteration 49000 with decaying accuracy 0.994664731642\n",
      "iteration 49200 with decaying accuracy 0.996190891005\n",
      "iteration 49400 with decaying accuracy 0.995641787464\n",
      "iteration 49600 with decaying accuracy 0.996766402765\n",
      "iteration 49800 with decaying accuracy 0.996518312271\n",
      "iteration 50000 with decaying accuracy 0.995789229341\n",
      "iteration 50200 with decaying accuracy 0.995981506765\n",
      "iteration 50400 with decaying accuracy 0.995341967103\n",
      "iteration 50600 with decaying accuracy 0.994174024402\n",
      "iteration 50800 with decaying accuracy 0.996442688558\n",
      "iteration 51000 with decaying accuracy 0.995875791873\n",
      "iteration 51200 with decaying accuracy 0.996281555405\n",
      "iteration 51400 with decaying accuracy 0.996455265277\n",
      "iteration 51600 with decaying accuracy 0.997189038198\n",
      "iteration 51800 with decaying accuracy 0.99720541667\n",
      "iteration 52000 with decaying accuracy 0.996213169739\n",
      "iteration 52200 with decaying accuracy 0.996614464945\n",
      "iteration 52400 with decaying accuracy 0.997363806864\n",
      "iteration 52600 with decaying accuracy 0.997078917566\n",
      "iteration 52800 with decaying accuracy 0.995437870776\n",
      "iteration 53000 with decaying accuracy 0.994698537642\n",
      "iteration 53200 with decaying accuracy 0.996061232975\n",
      "iteration 53400 with decaying accuracy 0.995429527273\n",
      "iteration 53600 with decaying accuracy 0.995278525123\n",
      "iteration 53800 with decaying accuracy 0.997643185124\n",
      "iteration 54000 with decaying accuracy 0.997599691212\n",
      "iteration 54200 with decaying accuracy 0.996341305658\n",
      "iteration 54400 with decaying accuracy 0.996017586878\n",
      "iteration 54600 with decaying accuracy 0.995113367764\n",
      "iteration 54800 with decaying accuracy 0.996418560853\n",
      "iteration 55000 with decaying accuracy 0.994942068815\n",
      "iteration 55200 with decaying accuracy 0.99525678979\n",
      "iteration 55400 with decaying accuracy 0.995462942237\n",
      "iteration 55600 with decaying accuracy 0.996343327463\n",
      "iteration 55800 with decaying accuracy 0.997280634579\n",
      "iteration 56000 with decaying accuracy 0.996650700531\n",
      "iteration 56200 with decaying accuracy 0.995931066214\n",
      "iteration 56400 with decaying accuracy 0.995651023146\n",
      "iteration 56600 with decaying accuracy 0.996509457726\n",
      "iteration 56800 with decaying accuracy 0.996142779421\n",
      "iteration 57000 with decaying accuracy 0.996347586148\n",
      "iteration 57200 with decaying accuracy 0.996053658647\n",
      "iteration 57400 with decaying accuracy 0.997709074301\n",
      "iteration 57600 with decaying accuracy 0.997509758609\n",
      "iteration 57800 with decaying accuracy 0.996908224012\n",
      "iteration 58000 with decaying accuracy 0.996206922015\n",
      "iteration 58200 with decaying accuracy 0.996336045965\n",
      "iteration 58400 with decaying accuracy 0.997569353195\n",
      "iteration 58600 with decaying accuracy 0.996934236638\n",
      "iteration 58800 with decaying accuracy 0.996804318338\n",
      "iteration 59000 with decaying accuracy 0.996667981074\n",
      "iteration 59200 with decaying accuracy 0.996111629139\n",
      "iteration 59400 with decaying accuracy 0.996316358102\n",
      "iteration 59600 with decaying accuracy 0.995051959797\n",
      "iteration 59800 with decaying accuracy 0.996106769137\n",
      "iteration 60000 with decaying accuracy 0.996762020716\n",
      "iteration 60200 with decaying accuracy 0.998329608638\n",
      "iteration 60400 with decaying accuracy 0.995642438792\n",
      "iteration 60600 with decaying accuracy 0.996186777361\n",
      "iteration 60800 with decaying accuracy 0.996761572532\n",
      "iteration 61000 with decaying accuracy 0.996263155589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 61200 with decaying accuracy 0.996422186159\n",
      "iteration 61400 with decaying accuracy 0.99628451306\n",
      "iteration 61600 with decaying accuracy 0.997194690723\n",
      "iteration 61800 with decaying accuracy 0.996122175969\n",
      "iteration 62000 with decaying accuracy 0.996766362843\n",
      "iteration 62200 with decaying accuracy 0.996745399503\n",
      "iteration 62400 with decaying accuracy 0.996969327594\n",
      "iteration 62600 with decaying accuracy 0.996535402525\n",
      "iteration 62800 with decaying accuracy 0.997537984282\n",
      "iteration 63000 with decaying accuracy 0.997681903836\n",
      "iteration 63200 with decaying accuracy 0.997558794791\n",
      "iteration 63400 with decaying accuracy 0.995297345069\n",
      "iteration 63600 with decaying accuracy 0.995871928779\n",
      "iteration 63800 with decaying accuracy 0.996819680709\n",
      "iteration 64000 with decaying accuracy 0.9974068338\n",
      "iteration 64200 with decaying accuracy 0.996333421355\n",
      "iteration 64400 with decaying accuracy 0.996646144818\n",
      "iteration 64600 with decaying accuracy 0.996807026263\n",
      "iteration 64800 with decaying accuracy 0.997644347\n",
      "iteration 65000 with decaying accuracy 0.997000182034\n",
      "iteration 65200 with decaying accuracy 0.997299623957\n",
      "iteration 65400 with decaying accuracy 0.997517885026\n",
      "iteration 65600 with decaying accuracy 0.995488805616\n",
      "iteration 65800 with decaying accuracy 0.995372744581\n",
      "iteration 66000 with decaying accuracy 0.995146153339\n",
      "iteration 66200 with decaying accuracy 0.996715630204\n",
      "iteration 66400 with decaying accuracy 0.997472705793\n",
      "iteration 66600 with decaying accuracy 0.996754511905\n",
      "iteration 66800 with decaying accuracy 0.997149780519\n",
      "iteration 67000 with decaying accuracy 0.996885896021\n",
      "iteration 67200 with decaying accuracy 0.998317254351\n",
      "iteration 67400 with decaying accuracy 0.997450583657\n",
      "iteration 67600 with decaying accuracy 0.996958180107\n",
      "iteration 67800 with decaying accuracy 0.997182592752\n",
      "iteration 68000 with decaying accuracy 0.996922534832\n",
      "iteration 68200 with decaying accuracy 0.997399165146\n",
      "iteration 68400 with decaying accuracy 0.997036176618\n",
      "iteration 68600 with decaying accuracy 0.997029246066\n",
      "iteration 68800 with decaying accuracy 0.996761696063\n",
      "iteration 69000 with decaying accuracy 0.995385222183\n",
      "iteration 69200 with decaying accuracy 0.997777089102\n",
      "iteration 69400 with decaying accuracy 0.99724170978\n",
      "iteration 69600 with decaying accuracy 0.997456547555\n",
      "iteration 69800 with decaying accuracy 0.996553432008\n",
      "iteration 70000 with decaying accuracy 0.997456681851\n",
      "iteration 70200 with decaying accuracy 0.996974850216\n",
      "iteration 70400 with decaying accuracy 0.994820029154\n",
      "iteration 70600 with decaying accuracy 0.997950789037\n",
      "iteration 70800 with decaying accuracy 0.997361711441\n",
      "iteration 71000 with decaying accuracy 0.996717715939\n",
      "iteration 71200 with decaying accuracy 0.997448855341\n",
      "iteration 71400 with decaying accuracy 0.997728871454\n",
      "iteration 71600 with decaying accuracy 0.997007863879\n",
      "iteration 71800 with decaying accuracy 0.997058090232\n",
      "iteration 72000 with decaying accuracy 0.998428813414\n",
      "iteration 72200 with decaying accuracy 0.996987376135\n",
      "iteration 72400 with decaying accuracy 0.995600236352\n",
      "iteration 72600 with decaying accuracy 0.997324286801\n",
      "iteration 72800 with decaying accuracy 0.997511453974\n",
      "iteration 73000 with decaying accuracy 0.998695830387\n",
      "iteration 73200 with decaying accuracy 0.997057276375\n",
      "iteration 73400 with decaying accuracy 0.996858669511\n",
      "iteration 73600 with decaying accuracy 0.995757489911\n",
      "iteration 73800 with decaying accuracy 0.995863567728\n",
      "iteration 74000 with decaying accuracy 0.997148710203\n",
      "iteration 74200 with decaying accuracy 0.99797554283\n",
      "iteration 74400 with decaying accuracy 0.997041530613\n",
      "iteration 74600 with decaying accuracy 0.995722119424\n",
      "iteration 74800 with decaying accuracy 0.996856885418\n",
      "iteration 75000 with decaying accuracy 0.99788291143\n",
      "iteration 75200 with decaying accuracy 0.998004131007\n",
      "iteration 75400 with decaying accuracy 0.996506107047\n",
      "iteration 75600 with decaying accuracy 0.997636112931\n",
      "iteration 75800 with decaying accuracy 0.997756913975\n",
      "iteration 76000 with decaying accuracy 0.996496505078\n",
      "iteration 76200 with decaying accuracy 0.996914344081\n",
      "iteration 76400 with decaying accuracy 0.997571475494\n",
      "iteration 76600 with decaying accuracy 0.99703991396\n",
      "iteration 76800 with decaying accuracy 0.997337535405\n",
      "iteration 77000 with decaying accuracy 0.997074111021\n",
      "iteration 77200 with decaying accuracy 0.997890799469\n",
      "iteration 77400 with decaying accuracy 0.99767200154\n",
      "iteration 77600 with decaying accuracy 0.996894153433\n",
      "iteration 77800 with decaying accuracy 0.998343338961\n",
      "iteration 78000 with decaying accuracy 0.996606072862\n",
      "iteration 78200 with decaying accuracy 0.998320218603\n",
      "iteration 78400 with decaying accuracy 0.997523738079\n",
      "iteration 78600 with decaying accuracy 0.997709613384\n",
      "iteration 78800 with decaying accuracy 0.997371047998\n",
      "iteration 79000 with decaying accuracy 0.996316240443\n",
      "iteration 79200 with decaying accuracy 0.996356097368\n",
      "iteration 79400 with decaying accuracy 0.996506558237\n",
      "iteration 79600 with decaying accuracy 0.996794612607\n",
      "iteration 79800 with decaying accuracy 0.997651935539\n",
      "iteration 80000 with decaying accuracy 0.997421880194\n",
      "iteration 80200 with decaying accuracy 0.997679707612\n",
      "iteration 80400 with decaying accuracy 0.997923598163\n",
      "iteration 80600 with decaying accuracy 0.997230125428\n",
      "iteration 80800 with decaying accuracy 0.998103897092\n",
      "iteration 81000 with decaying accuracy 0.997936747697\n",
      "iteration 81200 with decaying accuracy 0.996620780788\n",
      "iteration 81400 with decaying accuracy 0.997283233501\n",
      "iteration 81600 with decaying accuracy 0.998233922005\n",
      "iteration 81800 with decaying accuracy 0.997998999543\n",
      "iteration 82000 with decaying accuracy 0.996994561396\n",
      "iteration 82200 with decaying accuracy 0.997509490085\n",
      "iteration 82400 with decaying accuracy 0.997006712967\n",
      "iteration 82600 with decaying accuracy 0.99780109354\n",
      "iteration 82800 with decaying accuracy 0.996870791519\n",
      "iteration 83000 with decaying accuracy 0.9967442317\n",
      "iteration 83200 with decaying accuracy 0.997810123372\n",
      "iteration 83400 with decaying accuracy 0.997876820774\n",
      "iteration 83600 with decaying accuracy 0.99745586772\n",
      "iteration 83800 with decaying accuracy 0.997157953995\n",
      "iteration 84000 with decaying accuracy 0.997702509969\n",
      "iteration 84200 with decaying accuracy 0.997451285702\n",
      "iteration 84400 with decaying accuracy 0.997251400449\n",
      "iteration 84600 with decaying accuracy 0.998031189282\n",
      "iteration 84800 with decaying accuracy 0.998018726195\n",
      "iteration 85000 with decaying accuracy 0.997971697896\n",
      "iteration 85200 with decaying accuracy 0.997170346058\n",
      "iteration 85400 with decaying accuracy 0.996837022029\n",
      "iteration 85600 with decaying accuracy 0.997733332574\n",
      "iteration 85800 with decaying accuracy 0.99795214973\n",
      "iteration 86000 with decaying accuracy 0.997471436671\n",
      "iteration 86200 with decaying accuracy 0.996923408339\n",
      "iteration 86400 with decaying accuracy 0.996987260297\n",
      "iteration 86600 with decaying accuracy 0.996469288069\n",
      "iteration 86800 with decaying accuracy 0.998360301051\n",
      "iteration 87000 with decaying accuracy 0.997527162228\n",
      "iteration 87200 with decaying accuracy 0.997410500701\n",
      "iteration 87400 with decaying accuracy 0.998229481763\n",
      "iteration 87600 with decaying accuracy 0.997412084087\n",
      "iteration 87800 with decaying accuracy 0.998153124256\n",
      "iteration 88000 with decaying accuracy 0.995420367509\n",
      "iteration 88200 with decaying accuracy 0.997061337406\n",
      "iteration 88400 with decaying accuracy 0.996715989797\n",
      "iteration 88600 with decaying accuracy 0.997748715126\n",
      "iteration 88800 with decaying accuracy 0.997928403714\n",
      "iteration 89000 with decaying accuracy 0.998454264064\n",
      "iteration 89200 with decaying accuracy 0.998957328301\n",
      "iteration 89400 with decaying accuracy 0.998294805993\n",
      "iteration 89600 with decaying accuracy 0.998652564704\n",
      "iteration 89800 with decaying accuracy 0.997520699755\n",
      "iteration 90000 with decaying accuracy 0.997600218247\n",
      "iteration 90200 with decaying accuracy 0.996504681186\n",
      "iteration 90400 with decaying accuracy 0.997892831741\n",
      "iteration 90600 with decaying accuracy 0.997592612548\n",
      "iteration 90800 with decaying accuracy 0.997849717404\n",
      "iteration 91000 with decaying accuracy 0.996829504822\n",
      "iteration 91200 with decaying accuracy 0.997753939151\n",
      "iteration 91400 with decaying accuracy 0.997424599205\n",
      "iteration 91600 with decaying accuracy 0.99790090431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 91800 with decaying accuracy 0.998371144053\n",
      "iteration 92000 with decaying accuracy 0.996760068387\n",
      "iteration 92200 with decaying accuracy 0.997543302496\n",
      "iteration 92400 with decaying accuracy 0.996817168363\n",
      "iteration 92600 with decaying accuracy 0.997254426865\n",
      "iteration 92800 with decaying accuracy 0.996951230397\n",
      "iteration 93000 with decaying accuracy 0.998236699809\n",
      "iteration 93200 with decaying accuracy 0.997714886258\n",
      "iteration 93400 with decaying accuracy 0.997577022302\n",
      "iteration 93600 with decaying accuracy 0.998134395562\n",
      "iteration 93800 with decaying accuracy 0.997707656079\n",
      "iteration 94000 with decaying accuracy 0.997491557503\n",
      "iteration 94200 with decaying accuracy 0.997574510989\n",
      "iteration 94400 with decaying accuracy 0.998364725509\n",
      "iteration 94600 with decaying accuracy 0.99748630431\n",
      "iteration 94800 with decaying accuracy 0.996887035275\n",
      "iteration 95000 with decaying accuracy 0.997641852495\n",
      "iteration 95200 with decaying accuracy 0.998113740307\n",
      "iteration 95400 with decaying accuracy 0.997429249104\n",
      "iteration 95600 with decaying accuracy 0.998643057091\n",
      "iteration 95800 with decaying accuracy 0.997228156462\n",
      "iteration 96000 with decaying accuracy 0.997564492379\n",
      "iteration 96200 with decaying accuracy 0.997641885067\n",
      "iteration 96400 with decaying accuracy 0.998260205447\n",
      "iteration 96600 with decaying accuracy 0.997698730262\n",
      "iteration 96800 with decaying accuracy 0.996710454519\n",
      "iteration 97000 with decaying accuracy 0.998587017935\n",
      "iteration 97200 with decaying accuracy 0.997012942625\n",
      "iteration 97400 with decaying accuracy 0.997586260368\n",
      "iteration 97600 with decaying accuracy 0.998131396189\n",
      "iteration 97800 with decaying accuracy 0.997415464612\n",
      "iteration 98000 with decaying accuracy 0.998357003967\n",
      "iteration 98200 with decaying accuracy 0.998724334639\n",
      "iteration 98400 with decaying accuracy 0.997999551473\n",
      "iteration 98600 with decaying accuracy 0.997931003751\n",
      "iteration 98800 with decaying accuracy 0.998325127249\n",
      "iteration 99000 with decaying accuracy 0.997462352712\n",
      "iteration 99200 with decaying accuracy 0.997780018042\n",
      "iteration 99400 with decaying accuracy 0.997001936142\n",
      "iteration 99600 with decaying accuracy 0.99700024771\n",
      "iteration 99800 with decaying accuracy 0.996932759762\n",
      "iteration 100000 with decaying accuracy 0.997607995681\n",
      "iteration 100200 with decaying accuracy 0.998529309273\n",
      "iteration 100400 with decaying accuracy 0.998908802732\n",
      "iteration 100600 with decaying accuracy 0.998558573762\n",
      "iteration 100800 with decaying accuracy 0.996739728372\n",
      "iteration 101000 with decaying accuracy 0.997675964553\n",
      "iteration 101200 with decaying accuracy 0.99719099558\n",
      "iteration 101400 with decaying accuracy 0.996726705636\n",
      "iteration 101600 with decaying accuracy 0.996857124238\n",
      "iteration 101800 with decaying accuracy 0.997379129569\n",
      "iteration 102000 with decaying accuracy 0.998176401888\n",
      "iteration 102200 with decaying accuracy 0.997250559874\n",
      "iteration 102400 with decaying accuracy 0.996700556\n",
      "iteration 102600 with decaying accuracy 0.998706680471\n",
      "iteration 102800 with decaying accuracy 0.998647031003\n",
      "iteration 103000 with decaying accuracy 0.99819798221\n",
      "iteration 103200 with decaying accuracy 0.996977697756\n",
      "iteration 103400 with decaying accuracy 0.996816595356\n",
      "iteration 103600 with decaying accuracy 0.998190469752\n",
      "iteration 103800 with decaying accuracy 0.99795432362\n",
      "iteration 104000 with decaying accuracy 0.998423000284\n",
      "iteration 104200 with decaying accuracy 0.998963990659\n",
      "iteration 104400 with decaying accuracy 0.997936187428\n",
      "iteration 104600 with decaying accuracy 0.997691583404\n",
      "iteration 104800 with decaying accuracy 0.998092707128\n",
      "iteration 105000 with decaying accuracy 0.998326912625\n",
      "iteration 105200 with decaying accuracy 0.996629523111\n",
      "iteration 105400 with decaying accuracy 0.99798643125\n",
      "iteration 105600 with decaying accuracy 0.998479065225\n",
      "iteration 105800 with decaying accuracy 0.997612347971\n",
      "iteration 106000 with decaying accuracy 0.998550378136\n",
      "iteration 106200 with decaying accuracy 0.997124672979\n",
      "iteration 106400 with decaying accuracy 0.996663709061\n",
      "iteration 106600 with decaying accuracy 0.998660383015\n",
      "iteration 106800 with decaying accuracy 0.997544088645\n",
      "iteration 107000 with decaying accuracy 0.997609252403\n",
      "iteration 107200 with decaying accuracy 0.997256469941\n",
      "iteration 107400 with decaying accuracy 0.997766233223\n",
      "iteration 107600 with decaying accuracy 0.998401594755\n",
      "iteration 107800 with decaying accuracy 0.998508919512\n",
      "iteration 108000 with decaying accuracy 0.997888001111\n",
      "iteration 108200 with decaying accuracy 0.997869155894\n",
      "iteration 108400 with decaying accuracy 0.998493400861\n",
      "iteration 108600 with decaying accuracy 0.998081962444\n",
      "iteration 108800 with decaying accuracy 0.996953968454\n",
      "iteration 109000 with decaying accuracy 0.997515131094\n",
      "iteration 109200 with decaying accuracy 0.99719685847\n",
      "iteration 109400 with decaying accuracy 0.998065433483\n",
      "iteration 109600 with decaying accuracy 0.997483326381\n",
      "iteration 109800 with decaying accuracy 0.998499734547\n",
      "iteration 110000 with decaying accuracy 0.998113868372\n",
      "iteration 110200 with decaying accuracy 0.997840558628\n",
      "iteration 110400 with decaying accuracy 0.998757387253\n",
      "iteration 110600 with decaying accuracy 0.997943567366\n",
      "iteration 110800 with decaying accuracy 0.998150158394\n",
      "iteration 111000 with decaying accuracy 0.998520387663\n",
      "iteration 111200 with decaying accuracy 0.99863875634\n",
      "iteration 111400 with decaying accuracy 0.998757826561\n",
      "iteration 111600 with decaying accuracy 0.99815234824\n",
      "iteration 111800 with decaying accuracy 0.997277940564\n",
      "iteration 112000 with decaying accuracy 0.996745835356\n",
      "iteration 112200 with decaying accuracy 0.996003833574\n",
      "iteration 112400 with decaying accuracy 0.997341280785\n",
      "iteration 112600 with decaying accuracy 0.997669187691\n",
      "iteration 112800 with decaying accuracy 0.997282796851\n",
      "iteration 113000 with decaying accuracy 0.998341905663\n",
      "iteration 113200 with decaying accuracy 0.99742174456\n",
      "iteration 113400 with decaying accuracy 0.998887179556\n",
      "iteration 113600 with decaying accuracy 0.997882030789\n",
      "iteration 113800 with decaying accuracy 0.998130263963\n",
      "iteration 114000 with decaying accuracy 0.997485172551\n",
      "iteration 114200 with decaying accuracy 0.997856192743\n",
      "iteration 114400 with decaying accuracy 0.996801736297\n",
      "iteration 114600 with decaying accuracy 0.99823430556\n",
      "iteration 114800 with decaying accuracy 0.997670419569\n",
      "iteration 115000 with decaying accuracy 0.997333144366\n",
      "iteration 115200 with decaying accuracy 0.998176867528\n",
      "iteration 115400 with decaying accuracy 0.997308367773\n",
      "iteration 115600 with decaying accuracy 0.997685409616\n",
      "iteration 115800 with decaying accuracy 0.997182329494\n",
      "iteration 116000 with decaying accuracy 0.997793710173\n",
      "iteration 116200 with decaying accuracy 0.999049517194\n",
      "iteration 116400 with decaying accuracy 0.998033677544\n",
      "iteration 116600 with decaying accuracy 0.99770331143\n",
      "iteration 116800 with decaying accuracy 0.997493682716\n",
      "iteration 117000 with decaying accuracy 0.998408804063\n",
      "iteration 117200 with decaying accuracy 0.998095101826\n",
      "iteration 117400 with decaying accuracy 0.998725141783\n",
      "iteration 117600 with decaying accuracy 0.998373056083\n",
      "iteration 117800 with decaying accuracy 0.998436869644\n",
      "iteration 118000 with decaying accuracy 0.996962801413\n",
      "iteration 118200 with decaying accuracy 0.998316492351\n",
      "iteration 118400 with decaying accuracy 0.998474750585\n",
      "iteration 118600 with decaying accuracy 0.998675676344\n",
      "iteration 118800 with decaying accuracy 0.997912248665\n",
      "iteration 119000 with decaying accuracy 0.997199587115\n",
      "iteration 119200 with decaying accuracy 0.997619782597\n",
      "iteration 119400 with decaying accuracy 0.997534653896\n",
      "iteration 119600 with decaying accuracy 0.997258398949\n",
      "iteration 119800 with decaying accuracy 0.997911895435\n",
      "iteration 120000 with decaying accuracy 0.998491640053\n",
      "iteration 120200 with decaying accuracy 0.998023228658\n",
      "iteration 120400 with decaying accuracy 0.998181020624\n",
      "iteration 120600 with decaying accuracy 0.998280287429\n",
      "iteration 120800 with decaying accuracy 0.998071120843\n",
      "iteration 121000 with decaying accuracy 0.998058686233\n",
      "iteration 121200 with decaying accuracy 0.998205933506\n",
      "iteration 121400 with decaying accuracy 0.99893251367\n",
      "iteration 121600 with decaying accuracy 0.997500007822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 121800 with decaying accuracy 0.998172792037\n",
      "iteration 122000 with decaying accuracy 0.998277198364\n",
      "iteration 122200 with decaying accuracy 0.998147175918\n",
      "iteration 122400 with decaying accuracy 0.996782647124\n",
      "iteration 122600 with decaying accuracy 0.997472334477\n",
      "iteration 122800 with decaying accuracy 0.997519473631\n",
      "iteration 123000 with decaying accuracy 0.999201642562\n",
      "iteration 123200 with decaying accuracy 0.997814684743\n",
      "iteration 123400 with decaying accuracy 0.997120684604\n",
      "iteration 123600 with decaying accuracy 0.998196694431\n",
      "iteration 123800 with decaying accuracy 0.996883422876\n",
      "iteration 124000 with decaying accuracy 0.996677269469\n",
      "iteration 124200 with decaying accuracy 0.998060004809\n",
      "iteration 124400 with decaying accuracy 0.997429056656\n",
      "iteration 124600 with decaying accuracy 0.998424180869\n",
      "iteration 124800 with decaying accuracy 0.998734179628\n",
      "iteration 125000 with decaying accuracy 0.996509593949\n",
      "iteration 125200 with decaying accuracy 0.998052153785\n",
      "iteration 125400 with decaying accuracy 0.998401406452\n",
      "iteration 125600 with decaying accuracy 0.997806677603\n",
      "iteration 125800 with decaying accuracy 0.997622012631\n",
      "iteration 126000 with decaying accuracy 0.998355935502\n",
      "iteration 126200 with decaying accuracy 0.997883690079\n",
      "iteration 126400 with decaying accuracy 0.995872883969\n",
      "iteration 126600 with decaying accuracy 0.998393303528\n",
      "iteration 126800 with decaying accuracy 0.997811632481\n",
      "iteration 127000 with decaying accuracy 0.997466919227\n",
      "iteration 127200 with decaying accuracy 0.998330570278\n",
      "iteration 127400 with decaying accuracy 0.997609875724\n",
      "iteration 127600 with decaying accuracy 0.99850521078\n",
      "iteration 127800 with decaying accuracy 0.998551345098\n",
      "iteration 128000 with decaying accuracy 0.998133924852\n",
      "iteration 128200 with decaying accuracy 0.998566561554\n",
      "iteration 128400 with decaying accuracy 0.996900296042\n",
      "iteration 128600 with decaying accuracy 0.997453946179\n",
      "iteration 128800 with decaying accuracy 0.997787591594\n",
      "iteration 129000 with decaying accuracy 0.997660713583\n",
      "iteration 129200 with decaying accuracy 0.998522238635\n",
      "iteration 129400 with decaying accuracy 0.997921784248\n",
      "iteration 129600 with decaying accuracy 0.998160525777\n",
      "iteration 129800 with decaying accuracy 0.998308062072\n",
      "iteration 130000 with decaying accuracy 0.997821735831\n",
      "iteration 130200 with decaying accuracy 0.997078849921\n",
      "iteration 130400 with decaying accuracy 0.997684450411\n",
      "iteration 130600 with decaying accuracy 0.99775206377\n",
      "iteration 130800 with decaying accuracy 0.997593557342\n",
      "iteration 131000 with decaying accuracy 0.997253030529\n",
      "iteration 131200 with decaying accuracy 0.997491774771\n",
      "iteration 131400 with decaying accuracy 0.998568534589\n",
      "iteration 131600 with decaying accuracy 0.997390520467\n",
      "iteration 131800 with decaying accuracy 0.997570116027\n",
      "iteration 132000 with decaying accuracy 0.998090007138\n",
      "iteration 132200 with decaying accuracy 0.998336387445\n",
      "iteration 132400 with decaying accuracy 0.998068531944\n",
      "iteration 132600 with decaying accuracy 0.998351018521\n",
      "iteration 132800 with decaying accuracy 0.99809016001\n",
      "iteration 133000 with decaying accuracy 0.997886380302\n",
      "iteration 133200 with decaying accuracy 0.998177967171\n",
      "iteration 133400 with decaying accuracy 0.996905139625\n",
      "iteration 133600 with decaying accuracy 0.9971706311\n",
      "iteration 133800 with decaying accuracy 0.998631661048\n",
      "iteration 134000 with decaying accuracy 0.998910136537\n",
      "iteration 134200 with decaying accuracy 0.998064521491\n",
      "iteration 134400 with decaying accuracy 0.998179785414\n",
      "iteration 134600 with decaying accuracy 0.997827672965\n",
      "iteration 134800 with decaying accuracy 0.999089454674\n",
      "iteration 135000 with decaying accuracy 0.998231932743\n",
      "iteration 135200 with decaying accuracy 0.998723055143\n",
      "iteration 135400 with decaying accuracy 0.997972998536\n",
      "iteration 135600 with decaying accuracy 0.998619547567\n",
      "iteration 135800 with decaying accuracy 0.998461315783\n",
      "iteration 136000 with decaying accuracy 0.998637182388\n",
      "iteration 136200 with decaying accuracy 0.998194144948\n",
      "iteration 136400 with decaying accuracy 0.998514918754\n",
      "iteration 136600 with decaying accuracy 0.997601133883\n",
      "iteration 136800 with decaying accuracy 0.997355631587\n",
      "iteration 137000 with decaying accuracy 0.998231122949\n",
      "iteration 137200 with decaying accuracy 0.998497239741\n",
      "iteration 137400 with decaying accuracy 0.998735481498\n",
      "iteration 137600 with decaying accuracy 0.998982309613\n",
      "iteration 137800 with decaying accuracy 0.999155164428\n",
      "iteration 138000 with decaying accuracy 0.998839072454\n",
      "iteration 138200 with decaying accuracy 0.998070784575\n",
      "iteration 138400 with decaying accuracy 0.999063349299\n",
      "iteration 138600 with decaying accuracy 0.997800251644\n",
      "iteration 138800 with decaying accuracy 0.997828844856\n",
      "iteration 139000 with decaying accuracy 0.998146553461\n",
      "iteration 139200 with decaying accuracy 0.998293796794\n",
      "iteration 139400 with decaying accuracy 0.997355906793\n",
      "iteration 139600 with decaying accuracy 0.998420552445\n",
      "iteration 139800 with decaying accuracy 0.997413923038\n",
      "iteration 140000 with decaying accuracy 0.997830271853\n",
      "iteration 140200 with decaying accuracy 0.998117535935\n",
      "iteration 140400 with decaying accuracy 0.998565221806\n",
      "iteration 140600 with decaying accuracy 0.997898878375\n",
      "iteration 140800 with decaying accuracy 0.997780862624\n",
      "iteration 141000 with decaying accuracy 0.99679570801\n",
      "iteration 141200 with decaying accuracy 0.998453718925\n",
      "iteration 141400 with decaying accuracy 0.998780327138\n",
      "iteration 141600 with decaying accuracy 0.997827435186\n",
      "iteration 141800 with decaying accuracy 0.998483149703\n",
      "iteration 142000 with decaying accuracy 0.998624717775\n",
      "iteration 142200 with decaying accuracy 0.998267158326\n",
      "iteration 142400 with decaying accuracy 0.998937232575\n",
      "iteration 142600 with decaying accuracy 0.997485656093\n",
      "iteration 142800 with decaying accuracy 0.999052994186\n",
      "iteration 143000 with decaying accuracy 0.997510506936\n",
      "iteration 143200 with decaying accuracy 0.998188382699\n",
      "iteration 143400 with decaying accuracy 0.997893703933\n",
      "iteration 143600 with decaying accuracy 0.999102807391\n",
      "iteration 143800 with decaying accuracy 0.998877573979\n",
      "iteration 144000 with decaying accuracy 0.998101212304\n",
      "iteration 144200 with decaying accuracy 0.998208933447\n",
      "iteration 144400 with decaying accuracy 0.998516759732\n",
      "iteration 144600 with decaying accuracy 0.998673242715\n",
      "iteration 144800 with decaying accuracy 0.999183844543\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2fee8fab23c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatastream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mchar_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mcumulative\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcumulative\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cumulative = 0.35\n",
    "datastream = stream(joined, window_size, char_to_ind)(batch_size)\n",
    "for i in range(500000):\n",
    "    y, x = next(datastream)\n",
    "    acc = sess.run([accuracy, minimizer], feed_dict = { char_ids:x, labels:y, pkeep:0.5 })\n",
    "    cumulative *= 0.99\n",
    "    cumulative += 0.01 * acc[0] / batch_size\n",
    "    if i % 200 == 0:\n",
    "        print(\"iteration\", i, \"with decaying accuracy\", cumulative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.argmax(outputs, axis=2),\n",
    "accuracy = tf.reduce_sum(tf.cast(tf.equal(prediction, labels), tf.int64), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test internal accuracy using the same data set. We will use another data set to test generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 with total accuracy [ 0.23333333  0.3         0.43333333  0.56666667  0.6         0.56666667\n",
      "  0.7         0.66666667  0.7         0.76666667  0.73333333  0.73333333\n",
      "  0.76666667  0.73333333  0.73333333  0.8         0.83333333  0.83333333\n",
      "  0.9         0.9         0.93333333  0.93333333  0.93333333  0.96666667\n",
      "  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667\n",
      "  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667\n",
      "  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667\n",
      "  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667\n",
      "  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667\n",
      "  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667\n",
      "  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667\n",
      "  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667\n",
      "  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667\n",
      "  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667\n",
      "  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667\n",
      "  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667\n",
      "  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667\n",
      "  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667\n",
      "  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667\n",
      "  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667  0.96666667\n",
      "  1.          1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.        ]\n",
      "iteration 200 with total accuracy [ 0.16915423  0.25456053  0.31873964  0.38524046  0.44991708  0.5013267\n",
      "  0.55373134  0.60298507  0.64842454  0.68374793  0.71641791  0.74593698\n",
      "  0.7761194   0.80099502  0.81890547  0.84013267  0.85787728  0.87429519\n",
      "  0.88590381  0.89502488  0.90497512  0.91144279  0.92056385  0.92703151\n",
      "  0.9331675   0.93864013  0.94278607  0.94643449  0.95273632  0.95572139\n",
      "  0.95920398  0.96252073  0.96467662  0.96733002  0.9694859   0.97064677\n",
      "  0.97263682  0.97379768  0.97628524  0.97711443  0.97993367  0.98126036\n",
      "  0.98159204  0.98225539  0.98374793  0.98474295  0.98474295  0.98606965\n",
      "  0.98640133  0.986733    0.98789386  0.98855721  0.98922056  0.99004975\n",
      "  0.99054726  0.99170813  0.9920398   0.99253731  0.99270315  0.99270315\n",
      "  0.99353234  0.99320066  0.99353234  0.99369818  0.99402985  0.99436153\n",
      "  0.99402985  0.99436153  0.99436153  0.99436153  0.99485904  0.9946932\n",
      "  0.99502488  0.99485904  0.99552239  0.99585406  0.99552239  0.9960199\n",
      "  0.99651741  0.99618574  0.99635158  0.99701493  0.99684909  0.99701493\n",
      "  0.99701493  0.99701493  0.99767828  0.99751244  0.99767828  0.99784411\n",
      "  0.99834163  0.99834163  0.99817579  0.99834163  0.99817579  0.99800995\n",
      "  0.99800995  0.99800995  0.99817579  0.99834163  0.99834163  0.99834163\n",
      "  0.99834163  0.99834163  0.99850746  0.99834163  0.99850746  0.99850746\n",
      "  0.99850746  0.99850746  0.99850746  0.99850746  0.99850746  0.99834163\n",
      "  0.99834163  0.99850746  0.9986733   0.9986733   0.99850746  0.9986733\n",
      "  0.99883914  0.99883914  0.99917081  0.99917081  0.99900498  0.99917081\n",
      "  0.99900498  0.99933665  0.99933665  0.99917081]\n",
      "iteration 400 with total accuracy [ 0.17364921  0.25469659  0.32086451  0.3879468   0.45120532  0.50615129\n",
      "  0.55885287  0.60415628  0.64704904  0.68246052  0.71637573  0.74563591\n",
      "  0.77389859  0.79659185  0.81654198  0.83790524  0.85461347  0.87015794\n",
      "  0.88345802  0.89201995  0.90199501  0.91072319  0.91936825  0.92527016\n",
      "  0.93241895  0.93848712  0.94297589  0.94804655  0.95303408  0.95577722\n",
      "  0.95985037  0.96275977  0.9645054   0.96791355  0.96990856  0.97157107\n",
      "  0.97281796  0.97439734  0.97664173  0.97813799  0.98021613  0.98146301\n",
      "  0.98212801  0.98270989  0.98395677  0.98462178  0.98453865  0.98561929\n",
      "  0.98636741  0.98678304  0.98719867  0.98844555  0.98919368  0.98985869\n",
      "  0.99052369  0.9915212   0.9918537   0.9921862   0.99243558  0.99268495\n",
      "  0.99326683  0.99334996  0.99368246  0.99376559  0.99426434  0.99459684\n",
      "  0.99443059  0.99451372  0.99459684  0.99509559  0.99551122  0.99534497\n",
      "  0.99567747  0.99551122  0.99600998  0.99634248  0.9960931   0.99634248\n",
      "  0.99667498  0.9964256   0.99667498  0.99700748  0.99700748  0.99717373\n",
      "  0.99717373  0.99717373  0.99767249  0.99758936  0.99775561  0.99800499\n",
      "  0.99825436  0.99825436  0.99817124  0.99833749  0.99817124  0.99808811\n",
      "  0.99825436  0.99842062  0.99842062  0.99850374  0.99850374  0.99850374\n",
      "  0.99850374  0.99850374  0.99866999  0.99858687  0.99866999  0.99866999\n",
      "  0.99866999  0.99858687  0.99858687  0.99858687  0.99866999  0.99858687\n",
      "  0.99866999  0.99883624  0.99891937  0.99891937  0.99875312  0.99883624\n",
      "  0.99891937  0.99891937  0.99916874  0.99925187  0.99900249  0.99925187\n",
      "  0.99908562  0.999335    0.99916874  0.99916874]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-de43c87c11ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatastream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mchar_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mcum_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datastream = stream(joined, window_size, char_to_ind, error_rate=0.0)(batch_size)\n",
    "cum_acc = 0\n",
    "for i in range(3000):\n",
    "    y, x = next(datastream)\n",
    "    [acc] = sess.run([accuracy], feed_dict = { char_ids:x, labels: y, pkeep:1.0 })\n",
    "    cum_acc += acc[-1] / batch_size\n",
    "    if i % 200 == 0:\n",
    "        print(\"iteration\", i, \"with total accuracy\", cum_acc / (i + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the plot of the accuracy of the predictor. \n",
    "the x-axis is the length and the y-axis is the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4a32611d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([0, 130], [1, 1])\n",
    "plt.plot(cum_acc / 3000)\n",
    "plt.xlabel(\"sentence length\")\n",
    "plt.ylabel(\"prediction accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    if len(sentence) > window_size - 3:\n",
    "        raise ValueError\n",
    "    \n",
    "    new_s = \"| \" + sentence + \" \" * (window_size - len(sentence) - 2)\n",
    "    inputs = [char_to_ind[i] \n",
    "                if i in char_to_ind \n",
    "                else char_to_ind[\"error\"]\n",
    "                for i in new_s]\n",
    "    outvec = sess.run(outputs, feed_dict = { \n",
    "        char_ids: [inputs] * batch_size,\n",
    "        labels: [inputs] * batch_size,\n",
    "        pkeep: 1.0\n",
    "    })\n",
    "    outvec = -outvec[0, len(sentence) - 1]\n",
    "    # print(np.exp(outvec) / np.exp(outvec).sum())\n",
    "    return [languages[i] for i in np.argsort(outvec)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"./europarl.test\", \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    reader = [*reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "error ('da', 'sv', 'Det har fortfarande inte gjorts.')\n",
      "error ('de', 'en', 'Regarding Amendment No 48')\n",
      "error ('ro', 'fr', 'Je terminerai cependant sur un appel.')\n",
      "error ('sk', 'cs', 'Nesmme toto hledisko opomjet.')\n",
      "error ('sk', 'cs', 'Evropt oban mrhaj cennmi minutami, i dokonce hodinami cestovnm.')\n",
      "error ('sk', 'cs', 'Naprosto souhlasm s panem Maatenem.')\n",
      "0.994005994005994\n",
      "error ('pt', 'es', 'Pero afortunadamente s podemos hacer algo.')\n",
      "error ('sk', 'cs', 'lohou Ruska je zefektivnit sv postupy na hranicch.')\n",
      "error ('sk', 'cs', 'Standard, kter vyadovala Rada Evropy, byl pro vechny stejn a zvazn.')\n",
      "error ('sl', 'sk', 'Rozprava sa skonila.')\n",
      "error ('es', 'fr', 'Il ne dispose pas de fonds suffisants.')\n",
      "0.9945027486256871\n",
      "error ('de', 'nl', 'Water is echter niet gratis.')\n",
      "error ('sk', 'cs', 'Co tedy tato s obsahuje?')\n",
      "error ('it', 'fr', 'Le vote aura lieu mercredi  11h30.')\n",
      "error ('nl', 'da', 'Siden 1990 er verdenshandelen vokset eksplosivt.')\n",
      "error ('de', 'nl', 'Dit instrument is in dit opzicht dus volledig tekortgeschoten.')\n",
      "error ('lt', 'et', '(Parlament kinnitas ettepaneku.)')\n",
      "0.9943352215928024\n",
      "error ('sk', 'cs', 'Filipny (hlasovn)')\n",
      "error ('ro', 'fr', 'La directive \"habitat\" impose un objectif explicite.')\n",
      "error ('it', 'ro', 'Cauza este una nobil.')\n",
      "error ('pt', 'es', 'Puede tan slo declarar \"adoptado\" o \"no adoptado\".')\n",
      "error ('sk', 'cs', 'Tm myslm, e kdy jakkoli osoba, spolenost nebo banka chce investovat penze v daovm rji, mus daov rj deklarovat zemi pvodu penze, kter byly investovny.')\n",
      "error ('sk', 'cs', 'Rozhodnut o uritch dokumentech: viz zpis')\n",
      "error ('cs', 'sk', 'Vlda sa teraz o tento konflikt zaujma.')\n",
      "error ('sk', 'cs', 'Bianco ek pro budoucnost, kter zprva poaduje, nelze brt vn.')\n",
      "0.9937515621094727\n",
      "error ('pt', 'es', 'Es responsabilidad nuestra evitar esto.')\n",
      "error ('sk', 'cs', 'Rusko m zatm daleko k demokracii i k tomu stt se spolehlivm partnerem.')\n",
      "error ('hu', 'sk', 'Nasleduje hodina otzok (B6-0316/2007).')\n",
      "error ('sk', 'cs', 'Takov pohled je cynicky a nelidsk.')\n",
      "error ('nl', 'da', '(Protokollen godkendtes)')\n",
      "error ('sv', 'hu', 'A mondat nyelvtanilag nem helyes.')\n",
      "error ('en', 'nl', 'Op dit punt faalt Stockholm.')\n",
      "0.9936012797440512\n",
      "error ('sl', 'et', 'Eelmise istungi protokolli kinnitamine (vt protokoll)')\n",
      "error ('sk', 'cs', 'Dle mho nzoru se tyto innosti velice shoduj s vaimi nvrhy.')\n",
      "error ('da', 'sv', 'Under rens lopp har en tredjedel av befolkningen mist livet.')\n",
      "error ('sk', 'cs', 'dalm bodem je doba vyhrazen pro otzky (B6-0462/2008).')\n",
      "error ('sk', 'cs', 'Je tak nezbytn rozvinout vzkum v rmci 7. rmcovho programu.')\n",
      "0.9938343609398433\n",
      "error ('de', 'it', 'Relazione Hatzidakis (A5-0076/2000)')\n",
      "error ('sl', 'lt', \"(EL) Pone pirmininke, mes, Europos Parlamento PASOK frakcija, balsavome u S. Berlato's praneim.\")\n",
      "error ('hu', 'sk', 'Jedna Eurpa, jeden hlas!')\n",
      "error ('lt', 'lv', 'Balsojumu mutiskie skaidrojumi')\n",
      "error ('sk', 'cs', 'Nedolo k hloubkov analze rizik chyb v modelu maximlnho udritelnho vnosu.')\n",
      "error ('sk', 'cs', 'Pln prce: viz zpis')\n",
      "error ('it', 'es', '(El Presidente interrumpe a la oradora)')\n",
      "error ('da', 'sv', 'Det har nyss ppekats hur det kommer sig.')\n",
      "error ('sl', 'hu', 'Aggodalomra ad okot.')\n",
      "0.99342951006999\n",
      "error ('sk', 'sl', 'Prav tako bi rad spregovoril o pripombi Lszla Surjna.')\n",
      "error ('fi', 'fr', 'M. Seppnen en a parl hier.')\n",
      "error ('lv', 'lt', 'Jis, regis, kaip niekada ia tinka.')\n",
      "error ('sv', 'et', 'Aith, hrra Dehaene.')\n",
      "0.9937507811523559\n",
      "error ('de', 'sv', 'Herr ordfrande! Kollegor!')\n",
      "error ('sk', 'cs', 'Kad, kdo m zdrav rozum, je pacifista.')\n",
      "error ('da', 'sv', 'skriftlig. - (SK) Nivn p konsumentskyddet skiljer sig t mellan medlemsstaterna.')\n",
      "error ('en', 'fr', 'Naufrage du cargo New Flame (vote)')\n",
      "0.9940006665926008\n",
      "error ('es', 'en', 'Hence my plea, Mr President.')\n",
      "error ('cs', 'de', 'Und genau dort - in der historischen Region Tn, die heute die tschechische Stadt esk T und die polnische Stadt Cieszyn einschliet - wurde ein Projekt mit dem Namen \"Ein Garten an beiden Flussufern\" geschaffen, da ein Fluss zwischen den beiden Stdten verluft, die einst eine Stadteinheit bildeten.')\n",
      "error ('sl', 'sk', 'To boli pravdepodobne fakty.')\n",
      "error ('sk', 'cs', 'Tak tento odstavec chpu j.')\n",
      "error ('cs', 'sk', 'Je iba spolon mena.')\n",
      "0.9941005899410059\n",
      "error ('pt', 'sl', 'Gospa Breyer ima besedo.')\n",
      "error ('da', 'sv', 'Jag reserverar mig i tv frgor.')\n",
      "error ('fi', 'et', 'Ma vastan teie kahtlustele otse.')\n",
      "error ('hu', 'pt', 'Relatrio Thyssen (A4-0137/99)')\n",
      "error ('pt', 'es', 'Tengo una o dos preguntas.')\n",
      "error ('lt', 'lv', 'Pateicos jums visiem.')\n",
      "0.9940914462321607\n",
      "error ('pt', 'es', '(HU) Muchas gracias.')\n",
      "error ('en', 'da', 'Their will is the law, not only at home, but as to the concerns of every nation. ... They have swept away the very constitutions under which the Legislatures acted \" (De m vre mere end blinde, dem, som ikke kan se, med hvilken usvigelig systematik, i dette tilflde og i alle tilflde, de forflger deres plan for total delggelse af enhver uafhngig magt. ...')\n",
      "error ('lv', 'lt', 'Svarbu jau vien tai, kad i ataskaita paskelbta.')\n",
      "error ('sk', 'cs', 'To je podle mho nzoru koda.')\n",
      "error ('sk', 'cs', 'Synergie je dobr mylenka, je to dobr shrnut propojen politickch aktivit EU, politickch aktivit Komise, avak v zjmu lep strukturovanosti jsme j a pan Swoboda navrhli, aby j byl vdechnut parlamentn rozmr s konzultacemi ministr na rovni ernomosk oblasti.')\n",
      "error ('sv', 'et', '(Istung algas kell 9.00)')\n",
      "error ('sk', 'cs', 'Jedna skupina je na jasnch pravidlech zvl zvisl.')\n",
      "0.9940004999583368\n",
      "error ('sk', 'cs', 'Je to prce pro politiky lenskch stt.')\n",
      "error ('sk', 'cs', 'Jsou tyto zprvy pravdiv?')\n",
      "error ('lv', 'lt', 'A pats tai padariau.')\n",
      "error ('pl', 'sk', 'Rozsudok histrie je zrejm.')\n",
      "error ('fr', 'it', 'Le fantasie non servono!')\n",
      "0.9940773786631798\n",
      "error ('de', 'pl', 'Regiony te to belgijski region Limburg, holenderski region Limburg i region Aachen.')\n",
      "error ('en', 'fi', 'Me saamme oppia yhdess.')\n",
      "error ('en', 'da', 'Duro Barroso, har gjort for, at hr. Bush modtager Europa-Parlamentets formand, dvs.')\n",
      "error ('lt', 'en', '(BG) Thank you, Mr President.')\n",
      "error ('sk', 'cs', 'Rusk faktor je zsadn.')\n",
      "0.9941432754803229\n",
      "error ('sk', 'cs', 'Dnes myslm na ns, tedy na vs a na sebe.')\n",
      "error ('fr', 'en', 'Joint action is required.')\n",
      "error ('lv', 'et', 'Zoli, kas sinul oli sama?')\n",
      "error ('sk', 'cs', 'Jak jsem ji uvedl, tento nvrh byl ji v minulosti schvlen politickmi skupinami.')\n",
      "0.9942670488634091\n",
      "error ('lv', 'et', '(Juhataja katkestas kne)')\n",
      "error ('lv', 'lt', 'Suprantame, kad ikils problem.')\n",
      "error ('sk', 'cs', 'Pro mnoh lidi se ve Spojenm krlovstv ije v hojnosti.')\n",
      "error ('sk', 'cs', 'Dky jsem spolu s patncti spolenky nalezla svobodu.')\n",
      "error ('fi', 'et', 'See on meie eesmrk.')\n",
      "error ('et', 'fi', 'Varsinkin, jos te tarjoatte, jsen MacCormick.')\n",
      "0.9942503593525405\n",
      "error ('sk', 'cs', 'Jeho postaven by vak mohlo bt ohroeno, pokud by se jeho konkurenti uchlili k nekalm obchodnm praktikm nebo nedodrovali prva duevnho vlastnictv.')\n",
      "error ('de', 'cs', 'Jmenovali se William Meyer, Bernard Starie, Reginald Pike, Thomas Shaw, James McLeish, Archibald Barrowman a Albert Roberts a vichni budou v sobotu vyznamenni.')\n",
      "error ('lv', 'lt', 'Atminkite, Europa imeta tik 10 proc. CO2 kiekio.')\n",
      "error ('sl', 'it', 'Vi sono osservazioni?')\n",
      "0.9943532733368625\n",
      "error ('lv', 'lt', 'Pvz., C. Wortmann-Kool, D. Feio ir G. Papanikolaou tai nurodsavo kalbose.')\n",
      "error ('sk', 'cs', 'Nzev je velmi vstin.')\n",
      "error ('lv', 'lt', 'Tad kaip mums elgtis?')\n",
      "0.9945003055385812\n",
      "error ('sl', 'sk', 'To sme tu ete asi nemali.')\n",
      "error ('sk', 'cs', 'Co jsme tm tehdy dali najevo?')\n",
      "error ('cs', 'sk', 'Je to tak jednoduch.')\n",
      "error ('pl', 'ro', 'Mobilizarea Fondului european de ajustare la globalizare: H. Cegielski-Pozna, Polonia (')\n",
      "0.9945792326719647\n",
      "error ('sk', 'pt', 'Relatrio Pollack (A4-0161/98)')\n",
      "error ('en', 'da', 'Afbrydelse af sessionen')\n",
      "error ('cs', 'pl', 'A zatem problemy s.')\n",
      "0.9947002649867507\n",
      "error ('pt', 'sv', 'Vad berodde detta p?')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error ('hu', 'da', 'Vi skal tale med n stemme.')\n",
      "error ('lt', 'es', 'Todas juntas son positivas.')\n",
      "0.9947666602650278\n"
     ]
    }
   ],
   "source": [
    "count = len(reader)\n",
    "correct = 0\n",
    "errors = []\n",
    "random.shuffle(reader)\n",
    "for i, (lab, text) in enumerate(reader):\n",
    "    p = predict(text[:127])[0]\n",
    "    correct += lab == p[-2:]\n",
    "    if lab != p[-2:]:\n",
    "        print(\"error\", (p[-2:], lab, text))\n",
    "        errors.append((lab, text))\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(correct / (i + 1))\n",
    "print(correct / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correct, count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
