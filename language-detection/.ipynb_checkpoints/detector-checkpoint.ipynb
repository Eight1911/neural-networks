{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poom's C Char-RNN Langauge Detector\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(index, length):\n",
    "    \"\"\"\n",
    "    takes\n",
    "        two integers\n",
    "    returns\n",
    "        a onehot vector of length 'length'\n",
    "        with one at index 'index'\n",
    "    \"\"\"\n",
    "    vector = np.zeros(length)\n",
    "    vector[index] = 1.0\n",
    "    return vector\n",
    "\n",
    "def reader(directory):\n",
    "    \"\"\"\n",
    "    Takes \n",
    "        - directory : string\n",
    "            the directory path of all the text files\n",
    "    Returns \n",
    "        - dict[string -> list[string]]\n",
    "            a dictionary whose keys are the languages (folder-name)\n",
    "            and values are list of strings in the files in the folder\n",
    "    \"\"\"\n",
    "    def parse(base_name, files):\n",
    "        print(\"parsing files in\", base_name)\n",
    "        pattern = re.compile('<[^>]*>')\n",
    "        strings = []\n",
    "        for name in files:\n",
    "            with open(base_name + \"/\"+ name, errors='replace') as f:\n",
    "                strings.append(re.sub(pattern, '', f.read()))\n",
    "        return strings\n",
    "\n",
    "    def main(directory):\n",
    "        scanner = os.walk(directory)\n",
    "        _, folders, _ = next(scanner)\n",
    "        return {name: parse(name, files)\n",
    "                for name, _, files in scanner}\n",
    "    \n",
    "    return main(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we parse the files. This will take sometime. Go get coffee or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing files in ./text/bg\n",
      "parsing files in ./text/cs\n",
      "parsing files in ./text/da\n",
      "parsing files in ./text/de\n",
      "parsing files in ./text/el\n",
      "parsing files in ./text/en\n",
      "parsing files in ./text/es\n",
      "parsing files in ./text/et\n",
      "parsing files in ./text/fi\n",
      "parsing files in ./text/fr\n",
      "parsing files in ./text/hu\n",
      "parsing files in ./text/it\n",
      "parsing files in ./text/lt\n",
      "parsing files in ./text/lv\n",
      "parsing files in ./text/nl\n",
      "parsing files in ./text/pl\n",
      "parsing files in ./text/pt\n",
      "parsing files in ./text/ro\n",
      "parsing files in ./text/sk\n",
      "parsing files in ./text/sl\n",
      "parsing files in ./text/sv\n"
     ]
    }
   ],
   "source": [
    "strings = reader(\"./text\")\n",
    "joined = { lang: \"`\".join(strings[lang])\n",
    "            for lang in strings }\n",
    "charset = { lang: set(joined[lang]) for lang in joined }\n",
    "languages = sorted(charset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(languages, joined, window_size, char_to_ind):\n",
    "    \n",
    "    def next_point():\n",
    "        index = random.randrange(len(languages))\n",
    "        string = joined[languages[index]]\n",
    "        start = random.randrange(0, len(string) - window_size)\n",
    "        substring = string[start:start+window_size]\n",
    "        labels = [index] * window_size\n",
    "        inputs = np.array([char_to_ind[c] for c in substring])\n",
    "        return labels, inputs\n",
    "\n",
    "    def iterate(batch_size):\n",
    "        while True:\n",
    "            data = [next_point() for _ in range(batch_size)]\n",
    "            labels, inputs = zip(*data)\n",
    "            yield np.array(labels), np.array(inputs)\n",
    "\n",
    "    return iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4499268584 characters in 187072 files\n"
     ]
    }
   ],
   "source": [
    "num_chars = sum(len(text) for text in joined.values())\n",
    "num_files = sum(len(strings[lang]) for lang in strings)\n",
    "print(num_chars, \"characters in\", num_files, \"files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496 unique characters\n"
     ]
    }
   ],
   "source": [
    "unique_char = set()\n",
    "for lang in charset:\n",
    "    unique_char.update(charset[lang])\n",
    "\n",
    "unique_char = sorted(unique_char)\n",
    "num_unique_chars = len(unique_char)\n",
    "char_to_ind = { c : i for i, c in enumerate(unique_char) }\n",
    "char_to_vec = { c : onehot(i, len(unique_char)) \n",
    "                 for i, c in enumerate(unique_char)}\n",
    "print(num_unique_chars, \"unique characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a standard Char-RNN classifier for this problem. That is, we embed characters into $\\mathbb{R}^n$, feed the sequence of embedded points into an RNN and have each output be measured against the one-hot language vector using KL divergence.\n",
    "\n",
    "Since the data size is quite big relative to the batch size and rnn_size is small, it's not necessary to use dropout. But since random english appears in russian files, adding dropout will help with noisy labels. We also implemented https://arxiv.org/pdf/1705.03419.pdf to deal with this problem of noisy labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_size = 40\n",
    "num_layers = 2\n",
    "batch_size = 13\n",
    "window_size = 100\n",
    "output_size = 21\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_ids = tf.placeholder(tf.int64, [batch_size, window_size])\n",
    "labels   = tf.placeholder(tf.int64, [batch_size, window_size])\n",
    "pkeep    = tf.placeholder(tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.linspace(0, 1, window_size) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xavier initialization\n",
    "embedding = rd.randn(num_unique_chars, rnn_size)\n",
    "embedding = tf.Variable(embedding / np.sqrt(rnn_size + rnn_size))\n",
    "inp = tf.nn.embedding_lookup(embedding, char_ids)\n",
    "\n",
    "# xavier initialization\n",
    "noise_prob = tf.Variable(np.eye(output_size))\n",
    "decoder = rd.randn(rnn_size, output_size)\n",
    "decoder = tf.Variable(decoder / np.sqrt(rnn_size + output_size))\n",
    "bias    = tf.zeros(output_size, tf.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell = lambda size: (\n",
    "    tf.nn.rnn_cell.DropoutWrapper\n",
    "   (tf.nn.rnn_cell.LSTMCell(size), pkeep))\n",
    "lstm_cell = tf.nn.rnn_cell.LSTMCell\n",
    "cells = [lstm_cell(rnn_size) for _ in range(num_layers)]\n",
    "lstm = tf.nn.rnn_cell.MultiRNNCell(cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = lstm.zero_state(batch_size, tf.float64)\n",
    "outputs = []\n",
    "for i in range(window_size):\n",
    "    output, state = lstm(inp[:,i,:], state)\n",
    "    unscaled_logit = tf.matmul(output, decoder) + bias\n",
    "    unscaled_logit = tf.nn.softmax(unscaled_logit)\n",
    "    # see https://arxiv.org/pdf/1705.03419.pdf\n",
    "    denoiser = tf.matmul(unscaled_logit, noise_prob)\n",
    "    outputs.append(denoiser)\n",
    "    \n",
    "outputs = tf.stack(outputs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_index = -1\n",
    "prediction = tf.argmax(outputs[:, len_index], axis=1),\n",
    "accuracy = tf.reduce_sum(tf.cast(tf.equal(prediction, labels[:, len_index]), tf.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "longname = tf.nn.sparse_softmax_cross_entropy_with_logits # lol\n",
    "loss_matrix = longname(logits=outputs, labels=labels)\n",
    "total_loss = tf.reduce_mean(weights * loss_matrix)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "minimizer = optimizer.minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 with accuracy 0.3465\n",
      "iteration 200 with accuracy 0.193959385325\n",
      "iteration 400 with accuracy 0.219794536498\n",
      "iteration 600 with accuracy 0.241795761679\n",
      "iteration 800 with accuracy 0.260827810797\n",
      "iteration 1000 with accuracy 0.260836844854\n",
      "iteration 1200 with accuracy 0.249975477763\n",
      "iteration 1400 with accuracy 0.282224080663\n",
      "iteration 1600 with accuracy 0.290690208435\n",
      "iteration 1800 with accuracy 0.309968273729\n",
      "iteration 2000 with accuracy 0.328351172071\n",
      "iteration 2200 with accuracy 0.328672528058\n",
      "iteration 2400 with accuracy 0.334856344627\n",
      "iteration 2600 with accuracy 0.334587778115\n",
      "iteration 2800 with accuracy 0.369054941944\n",
      "iteration 3000 with accuracy 0.377971751971\n",
      "iteration 3200 with accuracy 0.392350487538\n",
      "iteration 3400 with accuracy 0.374985276068\n",
      "iteration 3600 with accuracy 0.37157121763\n",
      "iteration 3800 with accuracy 0.373034959424\n",
      "iteration 4000 with accuracy 0.384359196206\n",
      "iteration 4200 with accuracy 0.379678644765\n",
      "iteration 4400 with accuracy 0.389084105014\n",
      "iteration 4600 with accuracy 0.382652425554\n",
      "iteration 4800 with accuracy 0.400237775396\n",
      "iteration 5000 with accuracy 0.403237628317\n",
      "iteration 5200 with accuracy 0.408025154633\n",
      "iteration 5400 with accuracy 0.395918999513\n",
      "iteration 5600 with accuracy 0.389924459285\n",
      "iteration 5800 with accuracy 0.417787627602\n",
      "iteration 6000 with accuracy 0.407507827304\n",
      "iteration 6200 with accuracy 0.423202892846\n",
      "iteration 6400 with accuracy 0.419954320049\n",
      "iteration 6600 with accuracy 0.458902030727\n",
      "iteration 6800 with accuracy 0.44251852774\n",
      "iteration 7000 with accuracy 0.442669638198\n",
      "iteration 7200 with accuracy 0.451647772039\n",
      "iteration 7400 with accuracy 0.437758571687\n",
      "iteration 7600 with accuracy 0.454184077838\n",
      "iteration 7800 with accuracy 0.485689647898\n",
      "iteration 8000 with accuracy 0.506865498946\n",
      "iteration 8200 with accuracy 0.509423633571\n",
      "iteration 8400 with accuracy 0.518420556379\n",
      "iteration 8600 with accuracy 0.525290821234\n",
      "iteration 8800 with accuracy 0.511129206301\n",
      "iteration 9000 with accuracy 0.525372724288\n",
      "iteration 9200 with accuracy 0.537548053492\n",
      "iteration 9400 with accuracy 0.553399091805\n",
      "iteration 9600 with accuracy 0.547617289006\n",
      "iteration 9800 with accuracy 0.54315459418\n",
      "iteration 10000 with accuracy 0.535302700567\n",
      "iteration 10200 with accuracy 0.556294464552\n",
      "iteration 10400 with accuracy 0.56932179072\n",
      "iteration 10600 with accuracy 0.569022458272\n",
      "iteration 10800 with accuracy 0.560482340797\n",
      "iteration 11000 with accuracy 0.577372458012\n",
      "iteration 11200 with accuracy 0.57020170917\n",
      "iteration 11400 with accuracy 0.574589752593\n",
      "iteration 11600 with accuracy 0.611639592764\n",
      "iteration 11800 with accuracy 0.616284934413\n",
      "iteration 12000 with accuracy 0.621490687861\n",
      "iteration 12200 with accuracy 0.667166221214\n",
      "iteration 12400 with accuracy 0.675948676172\n",
      "iteration 12600 with accuracy 0.675752814127\n",
      "iteration 12800 with accuracy 0.678068559721\n",
      "iteration 13000 with accuracy 0.709665864723\n",
      "iteration 13200 with accuracy 0.724293823009\n",
      "iteration 13400 with accuracy 0.721245562255\n",
      "iteration 13600 with accuracy 0.732408346396\n",
      "iteration 13800 with accuracy 0.743828395924\n",
      "iteration 14000 with accuracy 0.748916026265\n",
      "iteration 14200 with accuracy 0.759219779683\n",
      "iteration 14400 with accuracy 0.780371195014\n",
      "iteration 14600 with accuracy 0.773948126219\n",
      "iteration 14800 with accuracy 0.786421427012\n",
      "iteration 15000 with accuracy 0.802356243826\n",
      "iteration 15200 with accuracy 0.806421163175\n",
      "iteration 15400 with accuracy 0.79333125787\n",
      "iteration 15600 with accuracy 0.800592367251\n",
      "iteration 15800 with accuracy 0.810517212094\n",
      "iteration 16000 with accuracy 0.807710096461\n",
      "iteration 16200 with accuracy 0.810814605077\n",
      "iteration 16400 with accuracy 0.825076512637\n",
      "iteration 16600 with accuracy 0.825642994904\n",
      "iteration 16800 with accuracy 0.835013083815\n",
      "iteration 17000 with accuracy 0.817501767456\n",
      "iteration 17200 with accuracy 0.831491191543\n",
      "iteration 17400 with accuracy 0.82896338775\n",
      "iteration 17600 with accuracy 0.838750660243\n",
      "iteration 17800 with accuracy 0.829765699101\n",
      "iteration 18000 with accuracy 0.844255296152\n",
      "iteration 18200 with accuracy 0.84133109795\n",
      "iteration 18400 with accuracy 0.836987100657\n",
      "iteration 18600 with accuracy 0.856858797334\n",
      "iteration 18800 with accuracy 0.84855038397\n",
      "iteration 19000 with accuracy 0.856877205715\n",
      "iteration 19200 with accuracy 0.859622267094\n",
      "iteration 19400 with accuracy 0.882359472684\n",
      "iteration 19600 with accuracy 0.866166498818\n",
      "iteration 19800 with accuracy 0.875997617549\n",
      "iteration 20000 with accuracy 0.886835953389\n",
      "iteration 20200 with accuracy 0.896784543\n",
      "iteration 20400 with accuracy 0.898993484273\n",
      "iteration 20600 with accuracy 0.896137816258\n",
      "iteration 20800 with accuracy 0.911680822319\n",
      "iteration 21000 with accuracy 0.903488831166\n",
      "iteration 21200 with accuracy 0.903050137017\n",
      "iteration 21400 with accuracy 0.908529367231\n",
      "iteration 21600 with accuracy 0.903134925661\n",
      "iteration 21800 with accuracy 0.921699459631\n",
      "iteration 22000 with accuracy 0.930318953187\n",
      "iteration 22200 with accuracy 0.927097321322\n",
      "iteration 22400 with accuracy 0.926732286563\n",
      "iteration 22600 with accuracy 0.938506102287\n",
      "iteration 22800 with accuracy 0.933521942159\n",
      "iteration 23000 with accuracy 0.934686229685\n",
      "iteration 23200 with accuracy 0.928724247865\n",
      "iteration 23400 with accuracy 0.939621796356\n",
      "iteration 23600 with accuracy 0.932587002896\n",
      "iteration 23800 with accuracy 0.940257013687\n",
      "iteration 24000 with accuracy 0.945040045156\n",
      "iteration 24200 with accuracy 0.941301701819\n",
      "iteration 24400 with accuracy 0.942485727709\n",
      "iteration 24600 with accuracy 0.945417290571\n",
      "iteration 24800 with accuracy 0.944682474576\n",
      "iteration 25000 with accuracy 0.944424536724\n",
      "iteration 25200 with accuracy 0.954526750967\n",
      "iteration 25400 with accuracy 0.94489130035\n",
      "iteration 25600 with accuracy 0.947467661586\n",
      "iteration 25800 with accuracy 0.948465932205\n",
      "iteration 26000 with accuracy 0.960320097888\n",
      "iteration 26200 with accuracy 0.95791977727\n",
      "iteration 26400 with accuracy 0.949888079027\n",
      "iteration 26600 with accuracy 0.954351788183\n",
      "iteration 26800 with accuracy 0.959348811074\n",
      "iteration 27000 with accuracy 0.954422213971\n",
      "iteration 27200 with accuracy 0.955042125953\n",
      "iteration 27400 with accuracy 0.95624855296\n",
      "iteration 27600 with accuracy 0.966311727006\n",
      "iteration 27800 with accuracy 0.966516238924\n",
      "iteration 28000 with accuracy 0.96161710174\n",
      "iteration 28200 with accuracy 0.958974709316\n",
      "iteration 28400 with accuracy 0.959649290233\n",
      "iteration 28600 with accuracy 0.966150158366\n",
      "iteration 28800 with accuracy 0.960209641321\n",
      "iteration 29000 with accuracy 0.960013002678\n",
      "iteration 29200 with accuracy 0.963588379496\n",
      "iteration 29400 with accuracy 0.963754030769\n",
      "iteration 29600 with accuracy 0.962421103344\n",
      "iteration 29800 with accuracy 0.965141433802\n",
      "iteration 30000 with accuracy 0.965743284393\n",
      "iteration 30200 with accuracy 0.961659239399\n",
      "iteration 30400 with accuracy 0.962009976141\n",
      "iteration 30600 with accuracy 0.965435389806\n",
      "iteration 30800 with accuracy 0.968406167097\n",
      "iteration 31000 with accuracy 0.974120956083\n",
      "iteration 31200 with accuracy 0.977642222933\n",
      "iteration 31400 with accuracy 0.967995495033\n",
      "iteration 31600 with accuracy 0.967777024619\n",
      "iteration 31800 with accuracy 0.969880173317\n",
      "iteration 32000 with accuracy 0.968550219285\n",
      "iteration 32200 with accuracy 0.974953656177\n",
      "iteration 32400 with accuracy 0.968741458553\n",
      "iteration 32600 with accuracy 0.972088028128\n",
      "iteration 32800 with accuracy 0.970130499138\n",
      "iteration 33000 with accuracy 0.975136481937\n",
      "iteration 33200 with accuracy 0.969087413507\n",
      "iteration 33400 with accuracy 0.972526160089\n",
      "iteration 33600 with accuracy 0.96917269007\n",
      "iteration 33800 with accuracy 0.978173454899\n",
      "iteration 34000 with accuracy 0.967585425563\n",
      "iteration 34200 with accuracy 0.970474769664\n",
      "iteration 34400 with accuracy 0.970269541381\n",
      "iteration 34600 with accuracy 0.97343658727\n",
      "iteration 34800 with accuracy 0.970142880415\n",
      "iteration 35000 with accuracy 0.97395585446\n",
      "iteration 35200 with accuracy 0.975754867493\n",
      "iteration 35400 with accuracy 0.973279901164\n",
      "iteration 35600 with accuracy 0.973412485542\n",
      "iteration 35800 with accuracy 0.974933236385\n",
      "iteration 36000 with accuracy 0.974060127708\n",
      "iteration 36200 with accuracy 0.97468963851\n",
      "iteration 36400 with accuracy 0.97752432274\n",
      "iteration 36600 with accuracy 0.975462476756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 36800 with accuracy 0.975074473643\n",
      "iteration 37000 with accuracy 0.969348762437\n",
      "iteration 37200 with accuracy 0.978024989382\n",
      "iteration 37400 with accuracy 0.971303742603\n",
      "iteration 37600 with accuracy 0.975863407537\n",
      "iteration 37800 with accuracy 0.97913879482\n",
      "iteration 38000 with accuracy 0.977636945738\n",
      "iteration 38200 with accuracy 0.975222032072\n",
      "iteration 38400 with accuracy 0.978903515074\n",
      "iteration 38600 with accuracy 0.985686057062\n",
      "iteration 38800 with accuracy 0.981980617502\n",
      "iteration 39000 with accuracy 0.974886563419\n",
      "iteration 39200 with accuracy 0.975182586038\n",
      "iteration 39400 with accuracy 0.979315209573\n",
      "iteration 39600 with accuracy 0.980977929781\n",
      "iteration 39800 with accuracy 0.984154268067\n",
      "iteration 40000 with accuracy 0.980057084959\n",
      "iteration 40200 with accuracy 0.978767113446\n",
      "iteration 40400 with accuracy 0.974359131256\n",
      "iteration 40600 with accuracy 0.979156882611\n",
      "iteration 40800 with accuracy 0.981158152787\n",
      "iteration 41000 with accuracy 0.981840583148\n",
      "iteration 41200 with accuracy 0.979343129939\n",
      "iteration 41400 with accuracy 0.981334102609\n",
      "iteration 41600 with accuracy 0.980487758995\n",
      "iteration 41800 with accuracy 0.982193768088\n",
      "iteration 42000 with accuracy 0.977343799493\n",
      "iteration 42200 with accuracy 0.983185648202\n",
      "iteration 42400 with accuracy 0.976705151643\n",
      "iteration 42600 with accuracy 0.975636691344\n",
      "iteration 42800 with accuracy 0.977185805849\n",
      "iteration 43000 with accuracy 0.984455786598\n"
     ]
    }
   ],
   "source": [
    "cumulative = 0.35\n",
    "datastream = stream(languages, joined, window_size, char_to_ind)(batch_size)\n",
    "for i in range(50000):\n",
    "    y, x = next(datastream)\n",
    "    acc, _ = sess.run([accuracy, minimizer], feed_dict = { char_ids:x, labels:y, pkeep:0.5 })\n",
    "    cumulative *= 0.99\n",
    "    cumulative += 0.01 * acc / batch_size\n",
    "    if i % 200 == 0:\n",
    "        print(\"iteration\", i, \"with decaying accuracy\", cumulative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5000):\n",
    "    y, x = next(datastream)\n",
    "    [acc] = sess.run([accuracy], feed_dict = { char_ids:x, labels:y, pkeep:1.0 })\n",
    "    cumulative += acc / batch_size\n",
    "    if i % 200 == 0:\n",
    "        print(\"iteration\", i, \"with decaying accuracy\", cumulative / (i + 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
